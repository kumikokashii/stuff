{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. RDD, DataFrame, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RDD = Resilient Distributed Datasets\n",
    "* RDDs can be significantly slower on Python compared with Java or Scala. DataFrames are not.\n",
    "* Dataset is type-specific so Python is not supported.\n",
    "* In Spark 2.0, DataFrame is Dataset Untyped API. (DataFrame = Dataset[Row])\n",
    "\n",
    "Tungsten is the project name to develop/improve the core engine of Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. RDD (schema-less ^_^)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext(appName='^_^')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tea', 777)]\n",
      "[['Michael Jackson', '8', '29', '1958'], ['David Bowie', '1', '8', '1947']]\n"
     ]
    }
   ],
   "source": [
    "# Create with python\n",
    "data = sc.parallelize([('tea', 777), ('coffee', 333), ('diet coke', 111), ('tea', 222)])\n",
    "print(data.take(1))\n",
    "\n",
    "# Read from a file. Can I use a zip file? It didn't work...\n",
    "data_from_file = sc.textFile('rockers.txt')\n",
    "parsed_data_from_file = data_from_file.map(lambda row: [x.strip() for x in row.split(',')])\n",
    "print(parsed_data_from_file.takeSample(False, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing examples:\n",
    "https://github.com/drabastomek/learningPySpark/blob/master/Chapter02/LearningPySpark_Chapter02.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1940', '1947', '1943', '1942', '1958']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at distinct values of a column\n",
    "rockers_born_years = parsed_data_from_file.map(lambda row: row[3]).distinct()\n",
    "rockers_born_years.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1443\n",
      "[('tea', 999), ('diet coke', 111), ('coffee', 333)]\n"
     ]
    }
   ],
   "source": [
    "# Reducing! Needs to be associative and commutative\n",
    "data_sum = data.map(lambda row: row[1]).reduce(lambda x, y: x + y)\n",
    "print(data_sum)\n",
    "\n",
    "data_sum_alt = data.reduceByKey(lambda x, y: x + y)\n",
    "print(data_sum_alt.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "dict_items([('diet coke', 1), ('tea', 2), ('coffee', 1)])\n"
     ]
    }
   ],
   "source": [
    "# Counting\n",
    "print(data.count())\n",
    "print(data.countByKey().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "* Can I change the key col to something other than the first column?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DataFrame (with schema!)\n",
    "* Let spark be pyspark.sql.SparkSession: https://spark.apache.org/docs/preview/api/python/pyspark.sql.html#pyspark.sql.SparkSession\n",
    "* Read from csv: http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext(appName='^_^')\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+--------+---------+\n",
      "|           name|born_month|born_day|born_year|\n",
      "+---------------+----------+--------+---------+\n",
      "|   Jimi Hendrix|        11|      27|     1942|\n",
      "|   Jim Morrison|        12|       8|     1943|\n",
      "|    John Lennon|        10|       9|     1940|\n",
      "|   Janis Joplin|         1|      19|     1943|\n",
      "|    Mick Jagger|         7|      26|     1943|\n",
      "|    David Bowie|         1|       8|     1947|\n",
      "|     Elton John|         3|      25|     1947|\n",
      "|Michael Jackson|         8|      29|     1958|\n",
      "|         Prince|         6|       7|     1958|\n",
      "+---------------+----------+--------+---------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- born_month: integer (nullable = true)\n",
      " |-- born_day: integer (nullable = true)\n",
      " |-- born_year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read from csv file by specifying each column's data type\n",
    "import pyspark.sql.types as typ\n",
    "\n",
    "schema = typ.StructType([typ.StructField('name', typ.StringType()), \n",
    "                         typ.StructField('born_month', typ.IntegerType()), \n",
    "                         typ.StructField('born_day', typ.IntegerType()), \n",
    "                         typ.StructField('born_year', typ.IntegerType())])\n",
    "\n",
    "df = spark.read.csv('rockers.csv', schema, header=True,\n",
    "                    ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True)\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+----+------+-----+\n",
      "|birth month|color|  id|  name|score|\n",
      "+-----------+-----+----+------+-----+\n",
      "|         12| blue|   0| Zelda|   99|\n",
      "|          8|green|  11|Tomato|   31|\n",
      "|          3| pink| 935|  Katy|   79|\n",
      "|          1|green|1000|  Tree|   88|\n",
      "|          9| pink|1001| Kitty| null|\n",
      "+-----------+-----+----+------+-----+\n",
      "\n",
      "root\n",
      " |-- birth month: long (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read from json-format rdd\n",
    "json_str_1 = '{\"id\": \"0\", \"name\": \"Zelda\", \"color\": \"blue\", \"birth month\": 12, \"score\": 99}'\n",
    "json_str_2 = '{\"id\": \"11\", \"name\": \"Tomato\", \"color\": \"green\", \"birth month\": 8, \"score\": 31}'\n",
    "json_str_3 = '{\"id\": \"935\", \"name\": \"Katy\", \"color\": \"pink\", \"birth month\": 3, \"score\": 79}'\n",
    "json_str_4 = '{\"id\": \"1000\", \"name\": \"Tree\", \"color\": \"green\", \"birth month\": 1, \"score\": 88}'\n",
    "json_str_5 = '{\"id\": \"1001\", \"name\": \"Kitty\", \"color\": \"pink\", \"birth month\": 9, \"score\": \"\"}'\n",
    "json_list = [json_str_1, json_str_2, json_str_3, json_str_4, json_str_5]\n",
    "\n",
    "rdd = sc.parallelize(json_list)\n",
    "df = spark.read.json(rdd)\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+---+------+-----+\n",
      "|birth month|color| id|  name|score|\n",
      "+-----------+-----+---+------+-----+\n",
      "|         12| blue|  0| Zelda|   99|\n",
      "|          8|green| 11|Tomato|   31|\n",
      "+-----------+-----+---+------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+------+-----------+-----+\n",
      "|  name|birth month|color|\n",
      "+------+-----------+-----+\n",
      "|Tomato|          8|green|\n",
      "+------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a temp table from DataFrame\n",
    "df.createOrReplaceTempView('df')\n",
    "\n",
    "# SQL query on the temp table!\n",
    "# Avoid .collect() because it will display all rows. Use .show(n) or .take(n)\n",
    "spark.sql('select * from df where `birth month` > 3').show(2)\n",
    "\n",
    "# Can also do querying with DataFrame API\n",
    "cols = ['name', 'birth month', 'color']\n",
    "df.select(*cols).filter('`birth month` > 3 and color = \"green\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+--------+----------+-----------+\n",
      "|birth month count|color count|id count|name count|score count|\n",
      "+-----------------+-----------+--------+----------+-----------+\n",
      "|                5|          3|       5|         5|          4|\n",
      "+-----------------+-----------+--------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count # of distinct values in each column\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df_agg = df.agg(*(countDistinct(col).alias(col + ' count') for col in df.columns))\n",
    "df_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+----+------+-----+-----------+\n",
      "|birth month|color|  id|  name|score|     new_id|\n",
      "+-----------+-----+----+------+-----+-----------+\n",
      "|         12| blue|   0| Zelda|   99|          0|\n",
      "|          8|green|  11|Tomato|   31| 8589934592|\n",
      "|          3| pink| 935|  Katy|   79|17179869184|\n",
      "|          1|green|1000|  Tree|   88|25769803776|\n",
      "|          9| pink|1001| Kitty| null|25769803777|\n",
      "+-----------+-----+----+------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add new id\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df_with_new_id = df.withColumn('new_id', monotonically_increasing_id())\n",
    "df_with_new_id.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+\n",
      "|summary|      birth month|             score|\n",
      "+-------+-----------------+------------------+\n",
      "|  count|                5|                 4|\n",
      "|   mean|              6.6|             74.25|\n",
      "| stddev|4.505552130427524|29.970819141291415|\n",
      "|    min|                1|                31|\n",
      "|    max|               12|                99|\n",
      "+-------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic stats for numerical values\n",
    "df.describe(['birth month', 'score']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Machine Learning\n",
    "* MLLib uses RDDs, but to be depreciated.\n",
    "* * Ex: https://github.com/drabastomek/learningPySpark/blob/master/Chapter05/LearningPySpark_Chapter05.ipynb\n",
    "* * Docu: http://spark.apache.org/docs/2.0.0/api/python/pyspark.mllib.html\n",
    "* ML uses DataFrames, but some things are still experimental.\n",
    "* * Ex:https://github.com/drabastomek/learningPySpark/blob/master/Chapter06/LearningPySpark_Chapter06.ipynb\n",
    "* * Docu: http://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
